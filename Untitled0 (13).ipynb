{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg9FnDKL1kmx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AWS Assignment\n",
        "\n",
        "* Understanding the difference between AWS Regions, Availability Zones, and Edge Locations is crucial for designing performant, reliable, and cost-effective cloud-based applications—especially for data analysis and latency-sensitive workloads.\n",
        "\n",
        "1. AWS Regions\n",
        "Definition: A Region is a geographic area that contains multiple, physically separated and isolated Availability Zones.\n",
        "\n",
        "Examples: us-east-1 (N. Virginia), eu-west-1 (Ireland), ap-southeast-1 (Singapore).\n",
        "\n",
        "Purpose: Allows users to deploy applications close to their users or to meet legal and compliance requirements regarding data residency.\n",
        "\n",
        "2. Availability Zones (AZs)\n",
        "Definition: Each Region consists of 2 or more AZs, which are physically separate data centers within the same region.\n",
        "\n",
        "Purpose: Designed for high availability and fault tolerance. If one AZ goes down (e.g., power outage), others can take over.\n",
        "\n",
        "Use Case: Deploying services across multiple AZs improves resilience (e.g., RDS Multi-AZ deployments, EC2 auto-scaling groups).\n",
        "\n",
        "3. Edge Locations\n",
        "Definition: Edge Locations are data centers in major cities around the world used by AWS services like CloudFront (CDN) and Route 53 (DNS).\n",
        "\n",
        "Purpose: Deliver content and services closer to users, improving performance by reducing latency.\n",
        "\n",
        "Use Case: Caching static assets (images, videos, etc.), accelerating API responses with CloudFront.\n",
        "\n",
        "Why This Matters for Data Analysis & Latency-Sensitive Applications\n",
        "Data Analysis\n",
        "Region Choice: Select Regions where data is collected or where compliance rules apply.\n",
        "\n",
        "AZ Use: For high availability in data pipelines (e.g., EMR clusters, Redshift, Glue).\n",
        "\n",
        "Data Transfer Costs: Transferring data across Regions is expensive—keeping it in-region reduces cost and latency.\n",
        "\n",
        "Latency-Sensitive Applications\n",
        "Edge Locations: Serve content via CDN from nearest edge for millisecond latency.\n",
        "\n",
        "Regions: Choose a region geographically close to your users for real-time systems (e.g., live analytics, gaming).\n",
        "\n",
        "AZs: Deploy across AZs to ensure that an outage doesn’t affect performance.\n",
        "\n",
        "* aws ec2 describe-regions --all-regions --query \"Regions[*].RegionName\" --output table\n",
        "\n",
        " Explanation:\n",
        "aws ec2 describe-regions: Retrieves information about all EC2 regions.\n",
        "\n",
        "--all-regions: Includes all regions, even those not enabled by default in your account.\n",
        "\n",
        "--query \"Regions[*].RegionName\": Filters the output to only show the region names.\n",
        "\n",
        "--output table: Formats the output as a readable table (you can also use json or text).\n",
        "\n",
        "---------------------------------\n",
        "|       DescribeRegions         |\n",
        "+-------------------------------+\n",
        "|  af-south-1                   |\n",
        "|  ap-east-1                    |\n",
        "|  ap-northeast-1              |\n",
        "|  ap-northeast-2              |\n",
        "|  ap-northeast-3              |\n",
        "|  ap-south-1                  |\n",
        "|  ap-south-2                  |\n",
        "|  ap-southeast-1              |\n",
        "|  ap-southeast-2              |\n",
        "|  ap-southeast-3              |\n",
        "|  ca-central-1                |\n",
        "|  eu-central-1                |\n",
        "|  eu-central-2                |\n",
        "|  eu-north-1                  |\n",
        "|  eu-south-1                  |\n",
        "|  eu-south-2                  |\n",
        "|  eu-west-1                   |\n",
        "|  eu-west-2                   |\n",
        "|  eu-west-3                   |\n",
        "|  me-central-1                |\n",
        "|  me-south-1                  |\n",
        "|  sa-east-1                   |\n",
        "|  us-east-1                   |\n",
        "|  us-east-2                   |\n",
        "|  us-west-1                   |\n",
        "|  us-west-2                   |\n",
        "+-------------------------------+\n",
        "\n",
        "\n",
        "*Step-by-Step (Using AWS CLI)\n",
        "Create IAM User:\n",
        "\n",
        "aws iam create-user --user-name s3-read-write-user\n",
        "\n",
        "\n",
        "Attach Inline Policy (Least Privilege):\n",
        "Here's a JSON policy that grants:\n",
        "\n",
        "Read and write access to only a specific bucket (example-bucket)\n",
        "\n",
        "No access to other services or buckets\n",
        "\n",
        "Least Privilege S3 Access Policy (JSON):\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6b1QQEiF11Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"Version\": \"2012-10-17\",\n",
        "  \"Statement\": [\n",
        "    {\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Action\": [\n",
        "        \"s3:GetObject\",\n",
        "        \"s3:PutObject\",\n",
        "        \"s3:DeleteObject\"\n",
        "      ],\n",
        "      \"Resource\": \"arn:aws:s3:::example-bucket/*\"\n",
        "    },\n",
        "    {\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Action\": [\n",
        "        \"s3:ListBucket\"\n",
        "      ],\n",
        "      \"Resource\": \"arn:aws:s3:::example-bucket\"\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fv4K9dm3A3m",
        "outputId": "4ba64b6e-4711-4e01-8adc-d4a964e84472"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Version': '2012-10-17',\n",
              " 'Statement': [{'Effect': 'Allow',\n",
              "   'Action': ['s3:GetObject', 's3:PutObject', 's3:DeleteObject'],\n",
              "   'Resource': 'arn:aws:s3:::example-bucket/*'},\n",
              "  {'Effect': 'Allow',\n",
              "   'Action': ['s3:ListBucket'],\n",
              "   'Resource': 'arn:aws:s3:::example-bucket'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attach the Policy:\n",
        "Save the JSON policy above in a file (e.g., s3-policy.json) and run:\n",
        "\n",
        "aws iam put-user-policy --user-name s3-read-write-user --policy-name S3LeastPrivilegePolicy --policy-document file://s3-policy.json\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KX_Gv4Qp3JC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use in Data Analytics Workflows\n",
        "1. S3 Standard\n",
        "Use when: You are actively using the data for ETL jobs, querying (e.g., Athena/Redshift Spectrum), or machine learning.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Data lakes used for daily dashboards\n",
        "\n",
        "Streaming pipeline output\n",
        "\n",
        "Training data for ML\n",
        "\n",
        "2. S3 Intelligent-Tiering\n",
        "Use when: Access patterns are unpredictable—S3 automatically moves objects between tiers to optimize cost.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Data exploration environments\n",
        "\n",
        "Ad hoc analytics queries\n",
        "\n",
        "Variable reporting workloads\n",
        "\n",
        "3. S3 Glacier Instant Retrieval\n",
        "Use when: You need to archive data but still want to retrieve it quickly for analytics or audits.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Archived transaction logs\n",
        "\n",
        "Old campaign data needed for comparison or back-testing\n",
        "\n",
        "4. S3 Glacier Flexible Retrieval\n",
        "Use when: Data is rarely needed, and retrieval speed isn't critical (minutes to hours is acceptable).\n",
        "\n",
        "Examples:\n",
        "\n",
        "Compliance logs\n",
        "\n",
        "Quarterly backups of processed datasets\n",
        "\n",
        "5. S3 Glacier Deep Archive\n",
        "Use when: Data is almost never accessed but must be stored for compliance or historical purposes.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Regulatory archives\n",
        "\n",
        "Long-term raw sensor data from IoT"
      ],
      "metadata": {
        "id": "X3svszPC3qII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step-by-Step: S3 Bucket with Versioning and File Versions\n",
        "1. Create an S3 Bucket\n",
        "\n",
        "aws s3api create-bucket --bucket my-sample-data-bucket-123456 \\\n",
        "    --region us-east-1 \\\n",
        "    --create-bucket-configuration LocationConstraint=us-east-1\n",
        "Replace my-sample-data-bucket-123456 with a unique name.\n",
        "\n",
        " 2. Enable Versioning\n",
        "\n",
        "aws s3api put-bucket-versioning --bucket my-sample-data-bucket-123456 \\\n",
        "    --versioning-configuration Status=Enabled\n",
        "3. Create and Upload a Sample CSV File (Version 1)\n",
        "Create a file:\n",
        "\n",
        "echo \"id,name,value\\n1,Alice,100\" > data.csv\n",
        "Upload it:\n",
        "\n",
        "\n",
        "aws s3 cp data.csv s3://my-sample-data-bucket-123456/data.c4. Modify and Upload the Same File (Version 2)\n",
        "Update the file:\n",
        "\n",
        "\n",
        "echo \"id,name,value\\n1,Alice,100\\n2,Bob,200\" > data.csv\n",
        "Upload again:\n",
        "\n",
        "aws s3 cp data.csv s3://my-sample-data-bucket-123456/data.csv\n",
        "5. List All Versions of the File\n",
        "bash\n",
        "aws s3api list-object-versions --bucket my-sample-data-bucket-123456 --prefix data.csv"
      ],
      "metadata": {
        "id": "wpRxmVye4B5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"Rules\": [\n",
        "    {\n",
        "      \"ID\": \"MoveToGlacierAndDelete\",\n",
        "      \"Prefix\": \"\",\n",
        "      \"Status\": \"Enabled\",\n",
        "      \"Transitions\": [\n",
        "        {\n",
        "          \"Days\": 30,\n",
        "          \"StorageClass\": \"GLACIER\"\n",
        "        }\n",
        "      ],\n",
        "      \"Expiration\": {\n",
        "        \"Days\": 90\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5c0XjhjOxfV",
        "outputId": "fe758765-df6a-460f-b7f6-05c9bfb5dcc2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Rules': [{'ID': 'MoveToGlacierAndDelete',\n",
              "   'Prefix': '',\n",
              "   'Status': 'Enabled',\n",
              "   'Transitions': [{'Days': 30, 'StorageClass': 'GLACIER'}],\n",
              "   'Expiration': {'Days': 90}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison Table: RDS vs DynamoDB vs Redshift\n",
        "Feature / Service\tAmazon RDS\tAmazon DynamoDB\tAmazon Redshift\n",
        "Type\tRelational database (SQL)\tNoSQL (key-value / document store)\tAnalytical data warehouse\n",
        "Best for\tOLTP (transactional workloads)\tLow-latency, high-throughput key-value access\tOLAP (complex analytics & reporting)\n",
        "Scaling\tVertical (limited horizontal with read replicas)\tHorizontal (auto scaling)\tMassively parallel (MPP)\n",
        "Schema\tFixed schema (SQL-based)\tSchema-less or flexible schema\tStructured schema (columnar)\n",
        "Latency\tMilliseconds\tSingle-digit milliseconds\tSeconds to minutes (batch queries)\n",
        "Data Size\tGBs to TBs\tKBs to TBs (scales well)\tTBs to PBs\n",
        "Cost Model\tInstance-based\tOn-demand or provisioned throughput\tStorage + compute (separate)\n",
        "\n",
        "When to Use in a Data Pipeline\n",
        "Stage\tService\tUse Case\n",
        "Ingestion + Processing\tDynamoDB\tReal-time ingestion of IoT device data for later analysis\n",
        "Transactional Storage\tRDS\tStoring cleaned and structured customer/order data from a web application\n",
        "Analytics + BI\tRedshift\tRunning daily/monthly dashboards and reports on aggregated sales data\n",
        "\n",
        "Use Case Details\n",
        "Amazon RDS\n",
        "Use Case: Store transactional data from a web or mobile app (e.g., customer orders).\n",
        "\n",
        "Why: Supports SQL, ACID compliance, joins, and complex relationships.\n",
        "\n",
        "Example: MySQL RDS stores user profiles, orders, and payment logs.\n",
        "\n",
        "Amazon DynamoDB\n",
        "Use Case: Capture real-time telemetry or clickstream data.\n",
        "\n",
        "Why: High throughput, low latency, and fully managed with built-in scaling.\n",
        "\n",
        "Example: DynamoDB stores millions of IoT sensor readings per minute for a smart factory.\n",
        "\n",
        "Amazon Redshift\n",
        "Use Case: Analyze historical sales trends or product performance.\n",
        "\n",
        "Why: Optimized for large-scale, complex queries over structured data.\n",
        "\n",
        "Example: Redshift aggregates product sales from the last 5 years for executive dashboards.\n",
        "\n"
      ],
      "metadata": {
        "id": "YaCZ83RZPLeV"
      }
    }
  ]
}